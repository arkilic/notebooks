{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import postgresql\n",
    "\n",
    "db = postgresql.open('pq://user:password@host:port/database')\n",
    "db.execute(\"CREATE TABLE emp (emp_first_name text, emp_last_name text, emp_salary numeric)\")\n",
    "make_emp = db.prepare(\"INSERT INTO emp VALUES ($1, $2, $3)\")\n",
    "make_emp(\"John\", \"Doe\", \"75,322\")\n",
    "with db.xact():\n",
    " make_emp(\"Jane\", \"Doe\", \"75,322\")\n",
    " make_emp(\"Edward\", \"Johnson\", \"82,744\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.execute(\"ALTER TABLE emp ADD COLUMN mycolumn character varying(50) DEFAULT 'whatever' NOT NULL;\")\n",
    "# Check alter performance\n",
    "# Check write/read performance\n",
    "# Check query performance\n",
    "# It is faster to create a new table from scratch than to update every single row. Sequential writes are faster than sparse updates and you don’t get dead rows at the end.\n",
    "# Table constraints and indexes heavily delay every write. If possible, you should drop all the indexes, triggers and foreign keys while the update runs and recreate them at the end.\n",
    "# Adding a nullable column without a default value is a cheap operation. Writing the actual data of the column is the expensive part.\n",
    "# Data stored in TOAST is not rewritten when the row is updated\n",
    "# Converting between some data types does not require a full table rewrite since Postgres 9.2. Ex: conversion from VARCHAR(32) to VARCHAR(64)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "from hive import ThriftHive\n",
    "from hive.ttypes import HiveServerException\n",
    "from thrift import Thrift\n",
    "from thrift.transport import TSocket\n",
    "from thrift.transport import TTransport\n",
    "from thrift.protocol import TBinaryProtocol\n",
    " \n",
    "try:\n",
    "    transport = TSocket.TSocket('localhost', 10000)\n",
    "    transport = TTransport.TBufferedTransport(transport)\n",
    "    protocol = TBinaryProtocol.TBinaryProtocol(transport)\n",
    " \n",
    "    client = ThriftHive.Client(protocol)\n",
    "    transport.open()\n",
    " \n",
    "    client.execute(\"CREATE TABLE r(a STRING, b INT, c DOUBLE)\")\n",
    "    client.execute(\"LOAD TABLE LOCAL INPATH '/path' INTO TABLE r\")\n",
    "    client.execute(\"SELECT * FROM r\")\n",
    "    while (1):\n",
    "      row = client.fetchOne()\n",
    "      if (row == None):\n",
    "        break\n",
    "      print row\n",
    "    client.execute(\"SELECT * FROM r\")\n",
    "    print client.fetchAll()\n",
    " \n",
    "    transport.close()\n",
    " \n",
    "except Thrift.TException, tx:\n",
    "    print '%s' % (tx.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQLite works great as the database engine for most low to medium traffic websites (which is to say, most websites). \n",
    "# The amount of web traffic that SQLite can handle depends on how heavily the website uses its database. \n",
    "# Generally speaking, any site that gets fewer than 100K hits/day should work fine with SQLite. \n",
    "# The 100K hits/day figure is a conservative estimate, not a hard upper bound. SQLite has been demonstrated to work\n",
    "# with 10 times that amount of traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Accumulo\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Parquet\t\n",
    "\n",
    "Decent python support\n",
    "\n",
    "Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EventStore\n",
    "\n",
    "An open-source, functional database with support for Complex Event Processing. It provides a persistence engine for applications using event-sourcing, or for storing time-series data. Event Store is written in C#, C++ for the server which runs on Mono or the .NET CLR, on Linux or Windows. Applications using Event Store can be written in JavaScript. Event sourcing (ES) is a way of persisting your application's state by storing the history that determines the current state of your application.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akiban Server\t\n",
    "Akiban Server is an open source database that brings document stores and relational databases together. Developers get powerful document access alongside surprisingly powerful SQL.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SenseiDB\t\n",
    "Open-source, distributed, realtime, semi-structured database. Some Features: Full-text search, Fast realtime updates, Structured and faceted search, BQL: SQL-like query language, Fast key-value lookup, High performance under concurrent heavy update and query volumes, Hadoop integration\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfluxDB\n",
    "InfluxDB is an open source distributed time series database with no external dependencies. It's useful for recording metrics, events, and performing analytics. It has a built-in HTTP API so you don't have to write any server side code to get up and running. InfluxDB is designed to be scalable, simple to install and manage, and fast to get data in and out. It aims to answer queries in real-time. That means every data point is indexed as it comes in and is immediately available in queries that should return under 100ms.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
