{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n",
      "Vendor:  Continuum Analytics, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    }
   ],
   "source": [
    "from __future__ import (absolute_import, print_function, division)\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.stats import dirichlet\n",
    "import numpy as np\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import norm\n",
    "# import numba\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from numpy.linalg import inv\n",
    "from numpy import log as ln\n",
    "from scipy.special import psi\n",
    "from numpy.random import random as rand\n",
    "%matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different than EM, variational inference introduces priors on model variables (local and global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wishart Distribution\n",
    "-----\n",
    "scipy.stats.wishart\n",
    "\n",
    "scipy.stats.wishart = <scipy.stats._multivariate.wishart_gen object at 0x4522576c>[source]\n",
    "A Wishart random variable.\n",
    "\n",
    "The df keyword specifies the degrees of freedom. The scale keyword specifies the scale matrix, which must be symmetric and positive definite. In this context, the scale matrix is often interpreted in terms of a multivariate normal precision matrix (the inverse of the covariance matrix).\n",
    "\n",
    "Parameters:\t\n",
    "x : array_like\n",
    "Quantiles, with the last axis of x denoting the components.\n",
    "df : int\n",
    "Degrees of freedom, must be greater than or equal to dimension of the scale matrix\n",
    "scale : array_like\n",
    "Symmetric positive definite scale matrix of the distribution\n",
    "random_state : None or int or np.random.RandomState instance, optional\n",
    "If int or RandomState, use it for drawing the random variates. If None (or np.random), the global np.random state is used. Default is None.\n",
    "Alternatively, the object may be called (as a function) to fix the degrees\n",
    "of freedom and scale parameters, returning a “frozen” Wishart random\n",
    "variable:\n",
    "rv = wishart(df=1, scale=1)\n",
    "Frozen object with the same methods but holding the given degrees of freedom and scale fixed.\n",
    "\n",
    "Notes\n",
    "\n",
    "The scale matrix scale must be a symmetric positive definite matrix. Singular matrices, including the symmetric positive semi-definite case, are not supported.\n",
    "\n",
    "The Wishart distribution is often denoted\n",
    "\n",
    "Wp(ν,Σ)\n",
    "where ν is the degrees of freedom and Σ is the p×p scale matrix.\n",
    "\n",
    "The probability density function for wishart has support over positive definite matrices S; if S∼Wp(ν,Σ), then its PDF is given by:\n",
    "\n",
    "$f(S)=|S|ν−p−122νp2|Σ|ν2Γp(ν2)exp(−tr(Σ−1S)/2)$\n",
    "If $S∼Wp(ν,Σ)$ (Wishart) then S−1∼W−1p(ν,Σ−1) (inverse Wishart).\n",
    "\n",
    "If the scale matrix is 1-dimensional and equal to one, then the Wishart distribution W1(ν,1) collapses to the χ2(ν) distribution.\n",
    "\n",
    "New in version 0.16.0.\n",
    "\n",
    "\n",
    "Dirichlet Distribution\n",
    "-------\n",
    "scipy.stats.dirichlet = <scipy.stats._multivariate.dirichlet_gen object at 0x452255cc>[source]\n",
    "A Dirichlet random variable.\n",
    "\n",
    "The alpha keyword specifies the concentration parameters of the distribution.\n",
    "\n",
    "New in version 0.15.0.\n",
    "\n",
    "Parameters:\t\n",
    "x : array_like\n",
    "Quantiles, with the last axis of x denoting the components.\n",
    "alpha : array_like\n",
    "The concentration parameters. The number of entries determines the dimensionality of the distribution.\n",
    "random_state : None or int or np.random.RandomState instance, optional\n",
    "If int or RandomState, use it for drawing the random variates. If None (or np.random), the global np.random state is used. Default is None.\n",
    "Alternatively, the object may be called (as a function) to fix\n",
    "concentration parameters, returning a “frozen” Dirichlet\n",
    "random variable:\n",
    "rv = dirichlet(alpha)\n",
    "Frozen object with the same methods but holding the given concentration parameters fixed.\n",
    "Each α entry must be positive. The distribution has only support on the simplex defined by\n",
    "\n",
    "$∑i=1Kxi≤1\n",
    "The probability density function for dirichlet is f(x)=1B(α)∏i=1Kxαi−1i\n",
    "where (α)=∏Ki=1Γ(αi)Γ(∑Ki=1αi)\n",
    "and α=(α1,…,αK), the concentration parameters and K is the dimension of the space where x takes values.$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From discussion on Wishart\n",
    "---\n",
    "* $E[\\Lambda] = aB^{-1}$ \n",
    "* $E[ln|\\Lambda|] = d(d-1)/4 * ln(\\pi) + d ln(2) - ln|B| + \\sum_{{j=1}}^{d}\\Psi(\\alpha/2+(1-j)/2)$\n",
    "\n",
    "From VI for GMM given distributions\n",
    "----\n",
    "* $q(\\pi) = Dirichlet(\\alpha_1^`, ..., \\alpha_K^`)$\n",
    "* $q(\\mu_j) = Normal(m_j^`,\\sum_j^`)$\n",
    "* $q(\\Lambda_j) = Wishart(\\alpha_j^`, B_j^`)$\n",
    "* $q(c_i) = Multinomial(\\phi_i)$\n",
    "\n",
    "The indices indicate the iterative dependence I am going to implement as a part of VI for GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpdf = wishart.pdf\n",
    "dirpdf = dirichlet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "* Data $x_1, x_2, ..., x_n$ where x $\\epsilon R^d$\n",
    "* k is the number of clusters as in GMM\n",
    "Output:\n",
    "* Parameters for $q(\\pi), q(\\mu_j), q(\\Lambda_j), q(c_i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VI Algorithm for Gaussian Mixture Model\n",
    "------\n",
    "1. Initialize $\\alpha_1^{(0)}, ...., \\alpha_k^{(0)}) , (m_j^{(0)}, \\sum_j^{(0)}), (\\alpha_j^{(0)}, B_j^{(0)})$ in some way.\n",
    "2. At iteration t:\n",
    "    * Update $q(c_i)$ for i=1, ..., n\n",
    "    * For j = 1, ...., K Set $n_j^{(t)}=\\sum_{j=1}^{d} \\phi_i^{t}(j)$\n",
    "    * For j = 1, ..., K \n",
    "        * Update $q(\\pi)$ by setting $\\alpha_j^{(t)} = \\alpha + n_j^{(t)}$\n",
    "    * For j = 1, ...., K\n",
    "        * Update $q(\\mu_j)$:\n",
    "            \n",
    "            * $\\sum_j^{(t)}$=$(c^{-1}I+n_j^{(t)}\\alpha_j^{(t-1)}(B_j^{(t-1)}))^{-1}$\n",
    "            * $m_j^{(t)}=\\sum_j^{(t)}(\\alpha_j^{(t-1)}(B_j^{(t-1)})^{-1}\\sum_{{i=1}}^{d}\\phi^{t}(j)x_i)$\n",
    "    * For j = 1, ..., K\n",
    "        * Update $q(\\Lambda_j)$:\n",
    "            \n",
    "            * $\\alpha_j^{(t)} = \\alpha + n_j^{(t)}$\n",
    "            * $B_j^{(t)}=B+\\sum_i^{n}\\phi_i^{(t)}(j)[(x_i-m_j^{(t})(x_i-m_j^{(t})^{T} + \\sum_j^{(t)}]$\n",
    "    * And lastly, calculate variation inference objective function L\n",
    "\n",
    "As one can see above, there are two major loops, 1 to T for iterations in order to update and 1 to K for j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t1(t1_j, a_j, b_j, t, d, j):\n",
    "    tmp = 0\n",
    "    for k in xrange(d):\n",
    "        tmp += psi((1 - d + a_j[t-1])/2)\n",
    "    t1_j[j] = tmp - np.log(b_j[t-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t2(t2_j, x_train, m_j, b_j, t, i, j):\n",
    "    x_m = x_train.values[i] - m_j[t-1]\n",
    "    a_B_j = a_j[t-1] * np.linalg.inv(b_j[t-1])\n",
    "    t2_j[j] = x_m.T* a_B_j * x_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t3(t3_j, a_j, b_j, eps_j, t, j):\n",
    "    t3_j[j] = np.trace(a_j[t-1] * np.linalg.inv(b_j[t-1]) * eps_j[t-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t4(t4_j, alpha_k, j, t):\n",
    "    tmp = 0\n",
    "    for l in range(k):\n",
    "        tmp += psi(alpha_k[t-1][l])\n",
    "    t4_j[j] = psi(alpha_k[t-1][j]) - tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_phi(t1_j, t2_j, t3_j, t4_j, j, i, t, n, k):\n",
    "    \n",
    "    # Step 2 (a)\n",
    "    t1_jk, t2_jk, t3_jk, t4_jk = np.ones((k,2,2)), np.ones((k, 2, 2)), np.ones(k), np.ones(k)\n",
    "    a_jk = np.full((T,2,2), fill_value=A_0)\n",
    "    b_jk = np.full((T,2,2), fill_value=B_0)\n",
    "    phi_ik = np.zeros((T, k, 2, 2))\n",
    "    n_jk = np.zeros(T)\n",
    "    alpha_kk = np.ones((T, k))\n",
    "    m_jk = np.random.rand(T, 2, 2)\n",
    "    eps_jk = np.random.rand(T, 2, 2)\n",
    "    phi_denum = 1\n",
    "    t1(t1_j, a_j, b_j, t, d, j)\n",
    "    t2(t2_j, x_train, m_j, b_j, t, i, j)\n",
    "    t3(t3_j, a_j, b_j, eps_j,t, j)\n",
    "    t4(t4_j, alpha_k, j, t)\n",
    "    phi_num = np.exp(0.5 * t1_j[j] - 0.5 \n",
    "                     * t2_j[j] - 0.5 * t3_j[j] + t4_j[j])    \n",
    "    for jj in xrange(k):\n",
    "        t1(t1_j, a_jk, b_jk, t, d, jj)\n",
    "        for ii in range(n):\n",
    "            t2(t2_jk, x_train, m_jk, b_jk, t, ii, jj)  \n",
    "        t3(t3_jk, a_j, b_j, eps_j, t,j)\n",
    "        t4(t4_jk, alpha_kk, jj, t)\n",
    "        phi_denum += np.exp(0.5 * t1_jk[jj] - 0.5 * t2_jk[jj] - 0.5 * t3_jk[jj] + t4_jk[jj])\n",
    "    \n",
    "    return phi_num/phi_denum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = E[ln p(x, c, \\pi, \\mu, \\Lambda)] - E[ln q]$\n",
    "\n",
    "$L = E[ln p(x|c, \\pi, \\mu, \\Lambda] + E[ln p(c|\\pi] - E[ln q]$\n",
    "\n",
    "$ E[ln p(x|c, \\pi, \\mu, \\Lambda)]$ will be $\\mu_i$\n",
    "\n",
    "$ x_i|c_i ~ Normal(\\mu_c \\Lambda_c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_func(t, b_j, a_j, m_j, n_j):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/arkilic/Desktop/')\n",
    "\n",
    "# Step 1\n",
    "\n",
    "x_train = pd.read_csv('data.txt', header=None)\n",
    "n, d = x_train.shape\n",
    "T = 2\n",
    "k = 2\n",
    "A_0 = x_train.cov() # empirical covariance\n",
    "t1_j, t2_j, t3_j, t4_j = np.ones((k,2,2)), np.ones((k, 2, 2)), np.ones(k), np.ones(k)\n",
    "B_0 = d/10 * A_0\n",
    "a_j = np.full((T,2,2), fill_value=A_0)\n",
    "b_j = np.full((T,2,2), fill_value=B_0)\n",
    "phi_i = np.zeros((T, k, 2, 2))\n",
    "n_j = np.zeros((T, 2, 2))\n",
    "alpha_k = np.random.rand(T, k)\n",
    "m_j = np.random.rand(T, 2, 2)\n",
    "eps_j = np.random.rand(T, 2, 2)\n",
    "\n",
    "for t in xrange(T):\n",
    "    for j in xrange(k):\n",
    "        # Step 2, part (b)\n",
    "        for i in xrange(n):\n",
    "            res = my_phi(t1_j, t2_j, t3_j, t4_j, j, i, t, n, k)\n",
    "            n_j[t] += res\n",
    "        p = isnan(n_j[t])\n",
    "        n_j[t][p] = 1/k\n",
    "        \n",
    "        # Step 2, part (c)\n",
    "        alpha_k[t] = 1 + np.diagonal(n_j[t])\n",
    "        # Step 3, part (d)\n",
    "        eps_j[t] = np.linalg.inv(1/10 * np.identity(2) + n_j[t] * a_j[t-1] \n",
    "                                         * np.linalg.inv(b_j[t-1]))\n",
    "    \n",
    "        m_j[t] = eps_j[t] * (a_j[t-1] * np.linalg.inv(b_j[t-1]) \n",
    "                                     * np.sum(phi_i[t] *  x_train.values[i]))\n",
    "        # Step 3, part (e)\n",
    "        a_j[t] = d + n_j[t]\n",
    "        \n",
    "        tmp2 = 0\n",
    "        for ix in xrange(n):\n",
    "            r  = my_phi(t1_j, t2_j, t3_j, t4_j, j, i, t, n, k)\n",
    "            tmp2 += r * (x_train.values[i] - m_j[t]) * (x_train.values[i] - m_j[t]).T * eps_j[t]\n",
    "        b_j[t] = tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
