{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from collections import deque\n",
    "from metadataclient.api import (insert_run_start, insert_descriptor,\n",
    "                                insert_run_stop, insert_event, bulk_insert_events)\n",
    "from metadataclient.api import (find_run_starts, find_descriptors, find_events,\n",
    "                               find_run_stops, find_last)\n",
    "from metadataclient.conf import connection_config\n",
    "from metadataclient.api import (insert_run_start, insert_descriptor, insert_event,\n",
    "                               bulk_insert_events, insert_run_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MdsMigrationException(RuntimeError):\n",
    "    \"\"\"Goodbye cruel world!\"\"\"\n",
    "    pass\n",
    "\n",
    "# Before this script is run, make sure you create a database called metadataservice\n",
    "\n",
    "# TODO: Switch to deque from OrderedDict if too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Ensure global connection management is functional by providing fake host information_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    MONGO_HOST = 'kusadasi'\n",
    "    connection_config['host'] = MONGO_HOST \n",
    "    res = next(find_last())\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connection Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MONGO_HOST = 'localhost'\n",
    "MONGO_PORT = 27017\n",
    "MIGRATION_DB = 'datastore'\n",
    "# assuming both mongo and tornado server on the same machine\n",
    "connection_config['host'] = MONGO_HOST \n",
    "connection_config['port'] = 7770\n",
    "connection_config['database'] = 'metadataservice'\n",
    "pymongo_client = MongoClient(MONGO_HOST, MONGO_PORT)\n",
    "database = pymongo_client[MIGRATION_DB]\n",
    "# try:\n",
    "#     res = (next(find_last()))\n",
    "# except Exception:\n",
    "#     raise MdsMigrationException('Seems like metadataservice is not accessible or empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Get all run_starts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database to be migrated Database(MongoClient('localhost', 27017), 'datastore')\n",
      "Database data being saved  localhost metadataservice\n"
     ]
    }
   ],
   "source": [
    "print('Database to be migrated', database) # visual sanity check\n",
    "print('Database data being saved ', connection_config['host'], connection_config['database'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rstart_oid_lookup(database, oid):\n",
    "    try:\n",
    "        res = next(database.run_start.find({'_id':oid}))\n",
    "    except StopIteration:\n",
    "        raise MdsMigrationException('Seems like run_start with this oid does not exist')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descriptor_oid_lookup(database, oid):\n",
    "    try:\n",
    "        res = next(database.event_descriptor.find({'_id':oid}))\n",
    "    except StopIteration:\n",
    "        raise MdsMigrationException('Seems like descriptor with this oid does not exist')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save all run_starts into an OrderedDict()**\n",
    "\n",
    "**Go over all beamline configurations, delete run_start reference fields, add as a embedded document to run_start, and update entry in run_starts deque()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'database': 'metadataservice', 'port': 7770, 'timezone': 'US/Eastern', 'protocol': 'http', 'host': 'localhost'}\n"
     ]
    }
   ],
   "source": [
    "rstart_crsr = database.run_start.find()\n",
    "run_starts = deque()\n",
    "print(connection_config)\n",
    "for rstart in rstart_crsr:\n",
    "    rs_id = rstart.pop('_id') # no leakeage from older version!\n",
    "    bcfg = database.beamline_config.find({'run_start': rs_id})\n",
    "    configs = {}\n",
    "    for b in bcfg:\n",
    "        del(b['_id'], b['run_start'])\n",
    "        configs[b['uid']] = b\n",
    "    tr1 = rstart.pop('time_as_datetime', None)\n",
    "    tr2 = rstart.pop('beamline_config_id', None)\n",
    "    params = dict(time=rstart.pop('time'), scan_id=rstart.pop('scan_id'),\n",
    "                 beamline_id=rstart.pop('beamline_id'), uid=rstart.pop('uid'), \n",
    "                 configs= (configs if configs else {}), migrated=True,\n",
    "                 **rstart)\n",
    "    run_starts.append(rstart)\n",
    "    insert_run_start(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get all descriptors and insert them using metadataclient.Iterate over descriptors, replace run_start oid references with run_start uid foreign keys, and save them to a deque and a dict that is keyed on uids(for caching)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Test this with an older schema\n",
    "desc_crsr = database.event_descriptor.find()\n",
    "descriptors = deque()\n",
    "desc_dict = {}\n",
    "invalid_descs = deque()\n",
    "for desc in desc_crsr:\n",
    "    rstart = rstart_oid_lookup(database, desc.pop('run_start_id'))\n",
    "    # make sure run_start exists\n",
    "    descr_id = desc.pop('_id') # clear the leaked oid field \n",
    "    desc['run_start'] = rstart['uid'] # overwrite the old foreign key\n",
    "    desc['migrated'] = True\n",
    "    tr3 = desc.pop('time_as_datetime', None)\n",
    "    try:\n",
    "        insert_descriptor(**desc)\n",
    "    except ValueError:\n",
    "        invalid_descs.append(desc)\n",
    "    desc['_id'] = descr_id\n",
    "    desc_dict[desc['uid']] = desc\n",
    "    descriptors.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# CSX Specific-- Migration created some None dtype that are consistently numbers\n",
    "print(len(invalid_descs))\n",
    "# Fix the None dtype fields in these invalid documents\n",
    "# Only 7 so it is not that bad\n",
    "for d in invalid_descs:\n",
    "    tb_inspec = d['data_keys']\n",
    "    for k, v in tb_inspec.items():\n",
    "        if v['dtype'] is None:\n",
    "            v['dtype'] = 'number' # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert all run_starts and descriptors so far**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pop descriptor from deque, get all corresponding events\n",
    "# bulk insert events at hand\n",
    "# Something is clearly wrong with the way I insert bulk_events. Tests pass but smth below is wrong\n",
    "# working on it\n",
    "for d in descriptors:\n",
    "    event_crsr = database.event.find({'descriptor_id': d.pop('_id')})\n",
    "    events = deque()\n",
    "    for e in event_crsr:\n",
    "        e['descriptor'] = d['uid']\n",
    "        e['migrated'] = True\n",
    "        del(e['_id'], e['descriptor_id'])\n",
    "        events.append(e)\n",
    "#         print(e)\n",
    "    bulk_insert_events(event_descriptor=d, events=events, validate=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rstop_crsr = database.run_stop.find()\n",
    "rstops = deque()\n",
    "for rstop in rstop_crsr:\n",
    "    del(rstop['_id'])\n",
    "    rstart = rstart_oid_lookup(database, rstop.pop('run_start_id'))\n",
    "    rstop['run_start'] = rstart['uid']\n",
    "    rstop['migrated'] = True\n",
    "    try:\n",
    "        insert_run_stop(**rstop)\n",
    "    except RuntimeError:\n",
    "        print('I caught duplicate run_stops! More than one run_stop per start!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
